---
title: "DATA_608_Final_Project"
author: "Joshua Sturm"
date: "May 1, 2018"
output:
  pdf_document:
    keep_tex: yes
  html_document:
    highlight: textmate
    theme: sandstone
    code_folding: show
    toc: yes
    toc_float: yes
    smart: yes
  github_document:
always_allow_html: yes
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, collapse = T)
setwd("~/GitHub/DATA_608/Final Project")
```

```{r load-libraries}
library(tidyverse)
library(magrittr)
library(reshape2)
```

```{r import-bysex, cache = T}
# bySex <- read.csv("./data/OB_PREV_by_sex_ALL_STATES.csv", na.strings = c("", "NA", "NaN", "No Data"), header = T)
# 
# # Any row that has an NA is missing the entire row, so we can safely remove those
# bySex <- bySex[complete.cases(bySex),]
# 
# colnames(bySex) <- c("State", "FIPS_Codes", "County",
#                      "numberMen", "percentMen", "lowerConfIntMen",
#                      "upperConfIntMen", "ageAdjPercMen", "ageAdjLowerConfIntMen",
#                      "ageAdjUpperConfIntMen", "numberWomen", "percentWomen",
#                      "lowerConfIntWomen", "upperConfIntWomen", "ageAdjPercWomen",
#                      "ageAdjLowerConfIntWomen", "ageAdjUpperConfIntWomen", "Year")
# write.csv(bySex, "bySex_all_states.csv")
```

```{r import-data, eval = F}
# cmb <- read.csv("./data/combined.csv")
cmb2 <- read.csv("./data/combined2.csv")
```

```{r melt-data, eval = F}
# k <- cmb %>%
#   gather("Sex", "count", 5:6)

k <- cmb2 %>%
  select(-c(FIPS_Code, oneRace, Multiracial, totPoverty25, totPoverty16to64, FamilyHousehold, OtherLivingArrangements,
            AmericanBorn, ForeignBorn, NaturalizedCitizen))
k %<>%
  melt(c(1:20, 24), variable.name = "Work", value.name = "wCount") %>%
  melt(c(1:18, 21:23), variable.name = "Disabled", value.name = "dCount") %>%
  # melt(c(1:17, 21:25), variable.name = "Citizenship", value.name = "cCount") %>%
  melt(c(1:14, 19:23), variable.name = "Education", value.name = "eCount") %>%
  melt(c(1:8, 15:21), variable.name = "Race", value.name = "rCount") %>%
  melt(c(1:5, 9:17), variable.name = "Age", value.name = "aCount") %>%
  melt(c(1:3, 6:16), variable.name = "Gender", value.name = "gCount") 
# write.csv(k, "melted2.csv")
```

```{r}
# library(data.table)
# a <- fread("./data/combined2.csv")
```

```{r}
# a %<>% 
#   select(-c(FIPS_Code, oneRace, Multiracial, totPoverty25, totPoverty16to64, FamilyHousehold, OtherLivingArrangements,
#             AmericanBorn, ForeignBorn, NaturalizedCitizen))
# 
# wcols <- c("FullTime", "PartTime", "NoWork")
# dcols <- c("Disability", "NoDisability")
# ecols <- c("NoHS", "HighSchool", "SomeCollege", "Bachelors")
# rcols <- c("White", "AfricanAmerican", "Native", "Asian", "Hawaiian", "otherRace")
# acols <- c("Under18", "a18to64", "over65")
# gcols <- c("Male", "Female")
# 
# nvals <- c("Work", "Disabled", "Education", "Race", "Age", "Gender")
# 
# m <- data.table::melt(a, measure = list(wcols, dcols, ecols, rcols, acols, gcols), value.name = nvals)
#   
```

As can be seen in the source files, I originally used multiple files to get all the information I wanted, before finding a [different source](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_16_1YR_S1703&prodType=table) that had all of it contained in a single table. I included the years 2010 through 2016.

Due to storage and memory constraints, I had to limit which columns I could use. I tried to choose those which I believed to provide the greatest insight. I did some basic tidying in Excel (such as removing / renaming columns) before loading the file to work with in javascript.

Since my only knowledge of javascript comes from this course, the next part took the bulk of my time. I first tried working directly with my data, before learning that I had to convert each column. I tried for hours to loop through all columns to convert automatically, but when nothing worked, I gave up and did each variable manually. 

Once I finally got the file loaded with d3, I ran into several problems with crossfilter. After toiling for *days*, I learned that crossfilter only accepts "long" data, and my file was wide. Through StackOverflow, I learned of something called melt.js, and finally I was able to get crossfilter working - for a single variable. I still haven't figured out how to filter a column in d3/crossfilter - or if it's even possible. 

Finally, I resorted to importing my file into R, and melting the entire file...multiple times. My reasoning was that I wanted each variable to have its own column and corresponding "count" column. There is most likely a better way to do this, but I haven't yet found one. The consequence of doing it this way was that each variable was repeated numerous times, and so the crossfilter output was really a multiple of the true values. Using R and Excel, I found out what that multiple was (it was different for each column), and divided by that. The numbers were now close to the real values, although they were in decimal format. When trying to round to an integer, I was getting large errors, which I assume is due to floating point arithmetic error.

The end result is that the dashboard...works. The file is quite large, so it takes ~10 seconds to load, but once it's committed to memory, crossfilter works *really* quickly; the calculations and transitions are near instantaneous. If I had more time, I would further enhance the display with CSS, but I fixed the position of a few graphs to make sure none of them overlap.

I could have made a really nice app in Plotly or Shiny in a fraction of the time (I spent 60+ hours on this project), but I wanted to take on this challenge. Upon completion of this project, and with a working demo, I am glad that I took the more difficult route. I may have missed a few deadlines because of it, but I learned so much from this struggle, that it made it all worthwhile.

Thank you, Professor Ferrari, for an amazing and educational semester!